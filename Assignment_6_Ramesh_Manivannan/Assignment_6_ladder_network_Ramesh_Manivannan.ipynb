{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ladder Network on CIFAR10 ###\n",
    "1.Iswariya Manivannan<br>\n",
    "2.Sathiya Ramesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.utils.data as utils_data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cuda0 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(cuda0)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = dsets.CIFAR10('/opt/datasets/cifar10', train=True, download=True, transform=transform)\n",
    "testset = dsets.CIFAR10('/opt/datasets/cifar10', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = utils_data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)\n",
    "testloader = utils_data.DataLoader(testset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinator function as per https://arxiv.org/pdf/1507.02672.pdf page 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combinator(nn.Module):\n",
    "    \n",
    "    def __init__(self, C):\n",
    "        \n",
    "        super(Combinator, self).__init__()\n",
    "        \n",
    "        self.b_0 = nn.Parameter(torch.zeros(1, C, 1, 1))\n",
    "        self.b_1 = nn.Parameter(torch.zeros(1, C, 1, 1))\n",
    "        self.w_0z = nn.Parameter(torch.ones(1, C, 1, 1))\n",
    "        self.w_1z = nn.Parameter(torch.ones(1, C, 1, 1))\n",
    "        self.w_0u = nn.Parameter(torch.zeros(1, C, 1, 1))\n",
    "        self.w_1u = nn.Parameter(torch.zeros(1, C, 1, 1))\n",
    "        self.w_0zu = nn.Parameter(torch.zeros(1, C, 1, 1))\n",
    "        self.w_1zu = nn.Parameter(torch.zeros(1, C, 1, 1))\n",
    "        self.w_sig = nn.Parameter(torch.ones(1, C, 1, 1))\n",
    "\n",
    "    def forward(self, zl, ul):\n",
    "        \n",
    "        \n",
    "        tem = self.b_1.repeat(zl.size(0), 1, 1, 1) + self.w_1z.repeat(zl.size(0), 1,1,1) * zl \\\n",
    "              + self.w_1u.repeat(zl.size(0), 1,1,1) * ul + self.w_1zu.repeat(zl.size(0), 1,1,1) * zl * ul\n",
    "        out = self.b_0.repeat(zl.size(0), 1,1,1) + self.w_0z.repeat(zl.size(0), 1,1,1) * zl + \\\n",
    "              self.w_0u.repeat(zl.size(0), 1,1,1) * ul + self.w_0zu.repeat(zl.size(0), 1,1,1) * zl * ul + tem.sigmoid_()\n",
    "        return out\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ladder Network architecture for CIFAR1 as per \"Conv-large model\" in https://arxiv.org/pdf/1507.02672.pdf page 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LadderNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LadderNetwork, self).__init__()\n",
    "        \n",
    "        self._noise_factor = 0.3\n",
    "        \n",
    "    #Encoder layers common for both noisy encoder and clean encoder\n",
    "    \n",
    "        self._bnorm0 = nn.BatchNorm2d(3)\n",
    "        self._conv1 = nn.Conv2d(3, 96, 3, 1, 1)\n",
    "        self._bnorm1 = nn.BatchNorm2d(96)\n",
    "        self._bnorm_corr_1 = nn.BatchNorm2d(96)\n",
    "        self._lrelu1 = nn.LeakyReLU()\n",
    "        \n",
    "        self._conv2 = nn.Conv2d(96, 96, 3, 1, 1)\n",
    "        self._bnorm2 = nn.BatchNorm2d(96)\n",
    "        self._bnorm_corr_2 = nn.BatchNorm2d(96)\n",
    "        self._lrelu2 = nn.LeakyReLU()\n",
    "        \n",
    "        self._conv3 = nn.Conv2d(96, 192, 3, 1, 1)\n",
    "        self._bnorm3 = nn.BatchNorm2d(192)\n",
    "        self._bnorm_corr_3 = nn.BatchNorm2d(192)\n",
    "        self._lrelu3 = nn.LeakyReLU()\n",
    "        self._mpool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        \n",
    "        self._conv4 = nn.Conv2d(192, 192, 3, 1, 1)\n",
    "        self._bnorm4 = nn.BatchNorm2d(192)\n",
    "        self._bnorm_corr_4 = nn.BatchNorm2d(192)\n",
    "        self._lrelu4 = nn.LeakyReLU()\n",
    "        \n",
    "        self._conv5 = nn.Conv2d(192, 192, 3, 1, 1)\n",
    "        self._bnorm5 = nn.BatchNorm2d(192)\n",
    "        self._bnorm_corr_5 = nn.BatchNorm2d(192)\n",
    "        self._lrelu5 = nn.LeakyReLU()\n",
    "        \n",
    "        self._conv6 = nn.Conv2d(192, 192, 3, 1, 1)\n",
    "        self._bnorm6 = nn.BatchNorm2d(192)\n",
    "        self._bnorm_corr_6 = nn.BatchNorm2d(192)\n",
    "        self._lrelu6 = nn.LeakyReLU()\n",
    "        self._mpool6 = nn.MaxPool2d(2)\n",
    "        \n",
    "        \n",
    "        self._conv7 = nn.Conv2d(192, 192, 3, 1, 1)\n",
    "        self._bnorm7 = nn.BatchNorm2d(192)\n",
    "        self._bnorm_corr_7 = nn.BatchNorm2d(192)\n",
    "        self._lrelu7 = nn.LeakyReLU()\n",
    "        \n",
    "        self._conv8 = nn.Conv2d(192, 192, 1, 1)\n",
    "        self._bnorm8 = nn.BatchNorm2d(192)\n",
    "        self._bnorm_corr_8 = nn.BatchNorm2d(192)\n",
    "        self._lrelu8 = nn.LeakyReLU()\n",
    "        \n",
    "        self._conv9 = nn.Conv2d(192, 10, 1, 1)\n",
    "        self._bnorm9 = nn.BatchNorm2d(10)\n",
    "        self._bnorm_corr_9 = nn.BatchNorm2d(10)\n",
    "        self._lrelu9 = nn.LeakyReLU()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Decoder layers\n",
    "        \n",
    "        self._tconv9 = nn.ConvTranspose2d(10, 192, 1, 1)\n",
    "        self._tlrelu9 = nn.LeakyReLU()\n",
    "        \n",
    "        self._tconv8 = nn.ConvTranspose2d(192, 192, 1, 1)\n",
    "        self._tlrelu8 = nn.LeakyReLU()\n",
    "        \n",
    "        self._tconv7 = nn.ConvTranspose2d(192, 192, 3, 1, 1)\n",
    "        self._tlrelu7 = nn.LeakyReLU()\n",
    "        \n",
    "        self._tconv6 = nn.ConvTranspose2d(192, 192, 3, 1, 1)\n",
    "        self._tlrelu6 = nn.LeakyReLU()\n",
    "\n",
    "        self._tconv5 = nn.ConvTranspose2d(192, 192, 3, 2, 1, 1)\n",
    "        self._tlrelu5 = nn.LeakyReLU()\n",
    "\n",
    "        self._tconv4 = nn.ConvTranspose2d(192, 192, 3, 2, 1, 1)\n",
    "        self._tlrelu4 = nn.LeakyReLU()\n",
    "\n",
    "        self._tconv3 = nn.ConvTranspose2d(192, 96, 3, 1, 1)\n",
    "        self._tlrelu3 = nn.LeakyReLU()\n",
    "        \n",
    "        self._tconv2 = nn.ConvTranspose2d(96, 96, 3, 1, 1)\n",
    "        self._tlrelu2 = nn.LeakyReLU()\n",
    "        \n",
    "        self._tconv1 = nn.ConvTranspose2d(96, 3, 3, 1, 1)\n",
    "        self._tlrelu1 = nn.LeakyReLU()\n",
    "        \n",
    "        self.comb0 = Combinator(3)\n",
    "        self.comb1 = Combinator(96)\n",
    "        self.comb2 = Combinator(96)\n",
    "        self.comb3 = Combinator(192)\n",
    "        self.comb4 = Combinator(192)\n",
    "        self.comb5 = Combinator(192)\n",
    "        self.comb6 = Combinator(192)\n",
    "        self.comb7 = Combinator(192)\n",
    "        self.comb8 = Combinator(192)\n",
    "        self.comb9 = Combinator(10)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def init_buffers(self):\n",
    "        \n",
    "        self.clean_encoder_output = []\n",
    "        self.noisy_encoder_output = []\n",
    "        self.decoder_output = []\n",
    "        self.encoder_mean = []\n",
    "        self.encoder_std = []\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Function to normalize decoder outputs\n",
    "    def normalize_decoder_output(self, x, mean, std):\n",
    "        \n",
    "        \n",
    "        x = x - mean.repeat(x.size(0), x.size(2), x.size(3), 1).permute(0,3,1,2)\n",
    "        x = x / (std.repeat(x.size(0),x.size(2), x.size(3), 1).permute(0,3,1,2) + 1e-5)\n",
    "        \n",
    "        return x  \n",
    "    \n",
    "       \n",
    "    def clean_encoder(self, x):\n",
    "        \n",
    "        \n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm0(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "    \n",
    "        \n",
    "        x = self._conv1(x)\n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm1(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "        x = self._bnorm_corr_1(x)\n",
    "        x = self._lrelu1(x)\n",
    "        \n",
    "        x = self._conv2(x)\n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm2(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "        x = self._bnorm_corr_2(x)\n",
    "        x = self._lrelu2(x)\n",
    "        \n",
    "        x = self._conv3(x)\n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm3(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "        x = self._bnorm_corr_3(x)\n",
    "        x = self._lrelu3(x)\n",
    "        x = self._mpool3(x)\n",
    "        \n",
    "        x = self._conv4(x)\n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm4(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "        x = self._bnorm_corr_4(x)\n",
    "        x = self._lrelu4(x)\n",
    "        \n",
    "        \n",
    "        x = self._conv5(x)\n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm5(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "        x = self._bnorm_corr_5(x)\n",
    "        x = self._lrelu5(x)\n",
    "        \n",
    "        x = self._conv6(x)\n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm6(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "        x = self._bnorm_corr_6(x)\n",
    "        x = self._lrelu6(x)\n",
    "        x = self._mpool6(x)\n",
    "        \n",
    "        x = self._conv7(x)\n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm7(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "        x = self._bnorm_corr_7(x)\n",
    "        x = self._lrelu7(x)\n",
    "        \n",
    "        x = self._conv8(x)\n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm8(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "        x = self._bnorm_corr_8(x)\n",
    "        x = self._lrelu8(x)\n",
    "        \n",
    "        x = self._conv9(x)\n",
    "        self.encoder_mean.append(x.mean(dim=0).mean(dim=1).mean(dim=1))\n",
    "        self.encoder_std.append(x.std(dim=0).mean(dim=1).mean(dim=1))\n",
    "        x = self._bnorm9(x)\n",
    "        self.clean_encoder_output.append(x.clone().detach())\n",
    "        x = self._bnorm_corr_9(x)\n",
    "        x = self._lrelu9(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    def noisy_encoder(self, x):\n",
    "        \n",
    "        \n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        \n",
    "        x = self._conv1(x)\n",
    "        x = self._bnorm1(x)\n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        x = self._bnorm_corr_1(x)\n",
    "        x = self._lrelu1(x)\n",
    "        \n",
    "        \n",
    "        x = self._conv2(x)\n",
    "        x = self._bnorm2(x)\n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        x = self._bnorm_corr_2(x)\n",
    "        x = self._lrelu2(x)\n",
    "        \n",
    "        \n",
    "        x = self._conv3(x)\n",
    "        x = self._bnorm3(x)\n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        x = self._bnorm_corr_3(x)\n",
    "        x = self._lrelu3(x)\n",
    "        x = self._mpool3(x)\n",
    "        \n",
    "        \n",
    "        x = self._conv4(x)\n",
    "        x = self._bnorm4(x)\n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        x = self._bnorm_corr_4(x)\n",
    "        x = self._lrelu4(x)\n",
    "        x = self._mpool6(x)\n",
    "        \n",
    "        x = self._conv5(x)\n",
    "        x = self._bnorm5(x)\n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        x = self._bnorm_corr_5(x)\n",
    "        x = self._lrelu5(x)\n",
    "        \n",
    "        x = self._conv6(x)\n",
    "        x = self._bnorm6(x)\n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        x = self._bnorm_corr_6(x)\n",
    "        x = self._lrelu6(x)\n",
    "        \n",
    "        x = self._conv7(x)\n",
    "        x = self._bnorm7(x)\n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        x = self._bnorm_corr_7(x)\n",
    "        x = self._lrelu7(x)\n",
    "        \n",
    "        x = self._conv8(x)\n",
    "        x = self._bnorm8(x)\n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        \n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        x = self._bnorm_corr_8(x)\n",
    "        x = self._lrelu8(x)\n",
    "        \n",
    "        \n",
    "        x = self._conv9(x)\n",
    "        x = self._bnorm9(x)\n",
    "        x = x + self._noise_factor * torch.randn(x.size()).to(cuda0)\n",
    "        self.noisy_encoder_output.append(x.clone())\n",
    "        x = self._bnorm_corr_9(x)\n",
    "        x = self._lrelu9(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    def decoder(self, x):\n",
    "        \n",
    "        \n",
    "        x = self._bnorm9(x)\n",
    "        \n",
    "        x = self.comb9.forward(self.noisy_encoder_output[9], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(),\n",
    "                                    self.encoder_mean[-1], self.encoder_std[-1]))\n",
    "        \n",
    "        \n",
    "        x = self._tconv9(x)\n",
    "        \n",
    "        x = self._tlrelu9(x)\n",
    "        x = self._bnorm8(x)\n",
    "        \n",
    "        x = self.comb8.forward(self.noisy_encoder_output[8], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n",
    "                                   self.encoder_mean[-2], self.encoder_std[-2]))\n",
    "        \n",
    "        \n",
    "        x = self._tconv8(x)\n",
    "        x = self._tlrelu8(x)\n",
    "        x = self._bnorm7(x)\n",
    "        \n",
    "        x = self.comb7.forward(self.noisy_encoder_output[7], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n",
    "                                   self.encoder_mean[-3], self.encoder_std[-3]))\n",
    "        \n",
    "        \n",
    "        x = self._tconv7(x)\n",
    "        x = self._tlrelu7(x)\n",
    "        x = self._bnorm6(x)\n",
    "        x = self.comb6.forward(self.noisy_encoder_output[6], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n",
    "                                   self.encoder_mean[-4], self.encoder_std[-4]))\n",
    "        \n",
    "        \n",
    "        x = self._tconv6(x)\n",
    "        x = self._tlrelu6(x)\n",
    "        x = self._bnorm5(x)\n",
    "        x = self.comb5.forward(self.noisy_encoder_output[5], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n",
    "                                   self.encoder_mean[-5], self.encoder_std[-5]))\n",
    "        \n",
    "        \n",
    "        x = self._tconv5(x)\n",
    "        x = self._tlrelu5(x)\n",
    "        x = self._bnorm4(x)\n",
    "        x = self.comb4.forward(self.noisy_encoder_output[4], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n",
    "                                   self.encoder_mean[-6], self.encoder_std[-6]))\n",
    "        \n",
    "        \n",
    "        x = self._tconv4(x)\n",
    "        x = self._tlrelu4(x)\n",
    "        x = self._bnorm3(x)\n",
    "        x = self.comb3.forward(self.noisy_encoder_output[3], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n",
    "                                   self.encoder_mean[-7], self.encoder_std[-7]))\n",
    "        \n",
    "        \n",
    "        x = self._tconv3(x)\n",
    "        x = self._tlrelu3(x)\n",
    "        x = self._bnorm2(x)\n",
    "        \n",
    "        x = self.comb2.forward(self.noisy_encoder_output[2], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n",
    "                                   self.encoder_mean[-8], self.encoder_std[-8]))\n",
    "        \n",
    "        \n",
    "        x = self._tconv2(x)\n",
    "        x = self._tlrelu2(x)\n",
    "        x = self._bnorm1(x)\n",
    "        \n",
    "        x = self.comb1.forward(self.noisy_encoder_output[1], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n",
    "                                   self.encoder_mean[-9], self.encoder_std[-9]))\n",
    "        \n",
    "        \n",
    "        x = self._tconv1(x)\n",
    "        x = self._tlrelu1(x)\n",
    "        \n",
    "        x = self.comb0.forward(self.noisy_encoder_output[0], x)\n",
    "        self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n",
    "                                   self.encoder_mean[-10], self.encoder_std[-10]))\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def forward(self, x, phase=\"Training\"):\n",
    "        \n",
    "        self.init_buffers()\n",
    "        \n",
    "        if phase==\"Test\":\n",
    "            \n",
    "            x = self.clean_encoder(orig_x)\n",
    "            y_hat = F.log_softmax(x)\n",
    "            \n",
    "            return y_hat\n",
    "        \n",
    "        orig_x = x.clone()\n",
    "        _ = self.clean_encoder(orig_x)\n",
    "        x = self.noisy_encoder(x)\n",
    "\n",
    "        y_hat = F.log_softmax(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return y_hat\n",
    "        \n",
    "        \n",
    "    def get_layer_values(self):\n",
    "        \n",
    "        return self.noisy_encoder_output, self.clean_encoder_output, self.decoder_output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train unlabelled data using the MSE loss. This is the reconstruction loss between the decoder and the clean encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unlabelled(model, optimizer, denoising_cost, **kwargs):\n",
    "    \n",
    "    \n",
    "    criterion = nn.MSELoss().to(cuda0)\n",
    "    \n",
    "    running_loss = 0\n",
    "    loss = 0.\n",
    "    \n",
    "    for i, (images, labels) in enumerate(kwargs['dataloader']):\n",
    "        \n",
    "        \n",
    "        # Loading data to GPU\n",
    "\n",
    "        Images = images.to(cuda0)\n",
    "        Labels = labels.to(cuda0)\n",
    "        \n",
    "        y_pred = model(Images)\n",
    "        noisy_encoder, clean_encoder, decoder = model.get_layer_values()\n",
    "\n",
    "        for cost, encoder, decoder in zip(denoising_cost, clean_encoder, reversed(decoder)):\n",
    "            \n",
    "            loss += cost * criterion(decoder, encoder)\n",
    "\n",
    "        running_loss += loss\n",
    "            \n",
    "        \n",
    "        if kwargs['phase'] == 'Training':\n",
    "            #Backward\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Iterations: {i}\")\n",
    "            \n",
    "\n",
    "            loss.backward(retain_graph = True) # Using retain_graph because of multiple forward passes\n",
    "            \n",
    "\n",
    "            #Update weights\n",
    "            optimizer.step()\n",
    "          \n",
    "    return running_loss/len(kwargs['dataloader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LadderNetwork().to(cuda0)\n",
    "learning_rate = 0.0001    \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imaniv2s/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:399: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\n",
      "Iterations: 1\n",
      "Iterations: 2\n",
      "Iterations: 3\n",
      "Iterations: 4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5091ea83c622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_unlabelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenoising_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_range_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_finder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Training set accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_loss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dc91e0a134f5>\u001b[0m in \u001b[0;36mtrain_unlabelled\u001b[0;34m(model, optimizer, denoising_cost, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnoisy_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-507e9b38a233>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, phase)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-507e9b38a233>\u001b[0m in \u001b[0;36mdecoder\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tlrelu5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bnorm4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomb4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoisy_encoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         self.decoder_output.append(self.normalize_decoder_output(x.clone(), \n\u001b[1;32m    344\u001b[0m                                    self.encoder_mean[-6], self.encoder_std[-6]))\n",
      "\u001b[0;32m<ipython-input-6-d54e6aad24ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, zl, ul)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mzl\u001b[0m               \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mul\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1zu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mzl\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_0z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mzl\u001b[0m \u001b[0;34m+\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_0u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mul\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_0zu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mzl\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mul\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "train_loss_values = []\n",
    "train_acc_values = []\n",
    "\n",
    "least_loss = 100\n",
    "\n",
    "\n",
    "denoising_cost = [1000, 10, 0.1, 0.1, 0.1]\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience =1)\n",
    "\n",
    "for epoch in range(25):\n",
    "    \n",
    "    train_loss, _ = train_unlabelled(model, optimizer, denoising_cost, dataloader = trainloader,  phase = 'Training', lr_range_val = [], lr_finder = False)  # Training set accuracy\n",
    "    train_loss_values.append(train_loss)\n",
    "    \n",
    "    if train_loss < least_loss:\n",
    "        least_loss = train_loss\n",
    "        encoder_checkpoint = copy.deepcopy(encoder.state_dict())\n",
    "        decoder_checkpoint = copy.deepcopy(decoder.state_dict())\n",
    "        \n",
    "    #if epoch % 5 ==0:\n",
    "    print(f'Epoch: {epoch}   Train Loss: {train_loss.cpu().numpy():.5f} ')\n",
    "        \n",
    "    scheduler.step(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with labelled and data uses a loss function which is a combination of both the reconstruction MSE loss and the cross-entropy loss. The output from the noisy encoder is compared with the true label in the cross-entropy loss and the MSE loss involves the output from the decoder and the clean encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, denoising_cost, **kwargs):\n",
    "    \n",
    "    \n",
    "    criterion_1 = nn.NLLLoss().to(cuda0) # Cost function for labelled data\n",
    "    criterion_2 = nn.MSELoss().to(cuda0)\n",
    "    \n",
    "    running_loss = 0\n",
    "\n",
    "    \n",
    "    for i, (images, labels) in enumerate(kwargs['dataloader']):\n",
    "        \n",
    "        \n",
    "        # Loading data to GPU\n",
    "\n",
    "        Images = images.to(cuda0)\n",
    "        Labels = labels.to(cuda0)\n",
    "        \n",
    "        y_pred = model(Images)\n",
    "        noisy_encoder, clean_encoder, decoder = model.get_layer_values()\n",
    "        \n",
    "        for cost, encoder, decoder in zip(denoising_cost, clean_encoder, reversed(decoder)):\n",
    "            \n",
    "            loss_1 += cost * criterion_2(decoder, encoder)\n",
    "            \n",
    "        loss_2 = criterion_1(y_pred, Labels)\n",
    "        \n",
    "        total_loss = loss_1 + loss_2\n",
    "        running_loss += loss.data[0]\n",
    "            \n",
    "        \n",
    "        if kwargs['phase'] == 'Training':\n",
    "            #Backward\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "\n",
    "            #Update weights\n",
    "            optimizer.step()\n",
    "          \n",
    "    return running_loss/len(kwargs['dataloader'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
